# ðŸ“˜ Machine Learning Roadmap (12-Month Plan)

This is a structured week-by-week ML learning plan with checklists.  
Goal: Go from fundamentals â†’ advanced ML â†’ research-level understanding.  
Track progress by checking off tasks as you complete them âœ….

---

## Phase 1: Foundations (Months 1â€“2)

### Week 1â€“2: Python for ML

- [ ] Learn Python basics (functions, classes, modules).
- [ ] Learn NumPy (arrays, broadcasting, linear algebra ops).
- [ ] Learn Pandas (dataframes, cleaning, manipulation).
- [ ] Learn Matplotlib/Seaborn (basic plotting).
- [ ] Mini-project: Load a dataset (Iris or Titanic) â†’ clean, analyze, visualize.

**Resources:**

- Python Data Science Handbook (Jake VanderPlas)
- Kaggle Python micro-course

---

### Week 3â€“6: Math Essentials

- [ ] Linear Algebra: vectors, matrices, dot product, eigenvalues, SVD.
- [ ] Probability: random variables, distributions, Bayes theorem.
- [ ] Statistics: expectation, variance, covariance, correlation.
- [ ] Calculus: derivatives, gradients, partial derivatives.
- [ ] Mini-project: Derive gradient descent for linear regression.

**Resources:**

- Mathematics for Machine Learning (Deisenroth et al.)
- Gilbert Strang MIT OCW lectures

---

### Week 7â€“8: Optimization Basics

- [ ] Understand convex vs non-convex optimization.
- [ ] Learn gradient descent, stochastic gradient descent (SGD).
- [ ] Learn momentum, RMSProp, Adam optimizers.
- [ ] Mini-project: Implement gradient descent from scratch in Python.

**Resources:**

- Convex Optimization (Boyd & Vandenberghe, selected chapters)

---

## Phase 2: Core ML (Months 3â€“5)

### Week 9â€“12: Supervised Learning

- [ ] Linear regression (closed form + gradient descent).
- [ ] Logistic regression (binary classification).
- [ ] Decision trees & Random Forests.
- [ ] Support Vector Machines (SVMs).
- [ ] Mini-project: Implement logistic regression + decision tree from scratch.

**Resources:**

- CS229 (Stanford ML course)
- Elements of Statistical Learning (Hastie et al.)

---

### Week 13â€“16: Unsupervised Learning

- [ ] K-means clustering.
- [ ] Principal Component Analysis (PCA).
- [ ] Gaussian Mixture Models (GMMs).
- [ ] Mini-project: Implement PCA and visualize dimensionality reduction.

**Resources:**

- Bishopâ€™s Pattern Recognition and Machine Learning

---

### Week 17â€“20: Model Evaluation & Regularization

- [ ] Cross-validation, train/test splits.
- [ ] Bias-variance tradeoff.
- [ ] Regularization: L1 (Lasso), L2 (Ridge).
- [ ] Mini-project: Kaggle Titanic dataset â†’ build baseline models.

---

## Phase 3: Deep Learning (Months 6â€“8)

### Week 21â€“24: Neural Network Basics

- [ ] Perceptrons, activation functions (ReLU, sigmoid, tanh).
- [ ] Backpropagation and chain rule.
- [ ] Loss functions (MSE, cross-entropy).
- [ ] Mini-project: Implement a 2-layer NN from scratch (no PyTorch).

**Resources:**

- Deep Learning (Goodfellow et al.), Ch. 6â€“8

---

### Week 25â€“28: Convolutional Neural Networks (CNNs)

- [ ] Convolutions, pooling, padding, stride.
- [ ] Architectures: LeNet, AlexNet, ResNet.
- [ ] Mini-project: Train a CNN on CIFAR-10 using PyTorch.

**Resources:**

- CS231n (Stanford)

---

### Week 29â€“32: Recurrent Neural Networks (RNNs) & LSTMs

- [ ] RNN basics, vanishing gradient problem.
- [ ] LSTMs, GRUs.
- [ ] Mini-project: Build an RNN for text classification.

**Resources:**

- CS224n (Stanford NLP)

---

## Phase 4: Advanced ML (Months 9â€“11)

### Week 33â€“36: Generative Models

- [ ] Variational Autoencoders (VAEs).
- [ ] Generative Adversarial Networks (GANs).
- [ ] Diffusion Models (intro).
- [ ] Mini-project: Implement a simple GAN in PyTorch.

---

### Week 37â€“40: Transformers & LLMs

- [ ] Read "Attention Is All You Need".
- [ ] Understand self-attention, multi-head attention.
- [ ] Learn encoder-decoder vs decoder-only architectures.
- [ ] Mini-project: Fine-tune a BERT model for sentiment analysis.

**Resources:**

- HuggingFace course

---

### Week 41â€“44: Reinforcement Learning

- [ ] Markov Decision Processes (MDPs).
- [ ] Q-learning, Deep Q-Networks (DQN).
- [ ] Policy gradients.
- [ ] Mini-project: Train an RL agent in OpenAI Gym.

**Resources:**

- Sutton & Bartoâ€™s Reinforcement Learning: An Introduction

---

## Phase 5: Research & Contribution (Month 12)

### Week 45â€“48: Paper Reading & Reproduction

- [ ] Read and summarize 3 seminal papers (ResNet, Transformer, Diffusion).
- [ ] Reproduce results in PyTorch.
- [ ] Compare your implementation with official repos.

---

### Week 49â€“52: Contribution & Sharing

- [ ] Contribute to an open-source ML repo (PyTorch, HuggingFace).
- [ ] Write a blog post explaining a concept you mastered.
- [ ] Start reading arXiv weekly (ML, AI, CV, NLP categories).
- [ ] Mini-project: Implement a recent arXiv paper.

---

# ðŸ“š Resource Pool (Side Quests)

Use these when you want to diverge:

- **Math Deep Dives:**

  - Linear Algebra Done Right (Axler)
  - Information Theory, Inference, and Learning Algorithms (MacKay)

- **ML/DL Books:**

  - Pattern Recognition and Machine Learning (Bishop)
  - Deep Learning (Goodfellow)

- **Courses:**

  - CS229 (ML), CS231n (CV), CS224n (NLP)
  - Deep Reinforcement Learning (Berkeley CS285)

- **Practice Platforms:**
  - Kaggle (competitions)
  - Papers With Code (implement SOTA models)

---

âœ… By the end of this plan, youâ€™ll be able to:

- Derive and implement ML/DL algorithms from scratch.
- Train and fine-tune state-of-the-art models.
- Read and reproduce research papers.
- Contribute to the ML community.
